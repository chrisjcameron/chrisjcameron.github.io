{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import dateutil.parser as dtp\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import bz2\n",
    "import io\n",
    "import heapq\n",
    "import functools\n",
    "\n",
    "import collections as coll\n",
    "import itertools as itr\n",
    "import regex as re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "#edit path the local minerva git repo\n",
    "# path_elements =[\n",
    "#     os.sep,\n",
    "#     'Users',\n",
    "#     'cjc73',\n",
    "#     'gits',\n",
    "#     'minerva',\n",
    "#     'detector',\n",
    "#     'swm',\n",
    "# ]\n",
    "# GIT_DB_PATH = os.path.abspath(os.path.join(*path_elements))\n",
    "# sys.path.append(GIT_DB_PATH)\n",
    "\n",
    "import swm_py3 as swm\n",
    "import tweet2token as tt\n",
    "import streamtools as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# helpers\n",
    "###################################################################\n",
    "\n",
    "def print_all(df):\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        print(df)\n",
    "\n",
    "\n",
    "def data_pusher(stream, consumer):\n",
    "    for item in stream:\n",
    "        #print(item)\n",
    "        consumer.send(item)\n",
    "    print(\"All done!\")\n",
    "    consumer.close()\n",
    "\n",
    "\n",
    "def file_streamer(in_file_path: str, skip=0):\n",
    "    with io.open(in_file_path, encoding='utf-8', newline='\\r\\n') as infile:\n",
    "        for i in range(skip):\n",
    "            infile.readline()\n",
    "        for line in infile:\n",
    "            yield line\n",
    "\n",
    "def linejson_file_streamer(in_file_path: str, skip=0):\n",
    "    with io.open(in_file_path, encoding='utf-8', newline='\\n') as infile:\n",
    "        for i in range(skip):\n",
    "            infile.readline()\n",
    "        for line in infile:\n",
    "            yield line\n",
    "            \n",
    "def bz2_linejson_file_streamer(in_file_path: str, skip=0):\n",
    "    with bz2.open(in_file_path, mode='rt', encoding='utf-8', newline='\\n') as infile:\n",
    "        for i in range(skip):\n",
    "            infile.readline()\n",
    "        for line in infile:\n",
    "            yield line\n",
    "            \n",
    "def print_stream():\n",
    "    while True:\n",
    "        to_print = (yield)\n",
    "        if to_print is not None:\n",
    "            print(to_print)\n",
    "\n",
    "def get_tid():\n",
    "    \"\"\"Extract tid from a tweet\"\"\"def verbose_yield(func):\n",
    "    def in_fn(*args, **kwargs):\n",
    "        print(f\"{func.__name__} got {list(args)}\") \n",
    "        res = func(*args, **kwargs)\n",
    "        print(res)\n",
    "        yield res\n",
    "    return in_fn\n",
    "    return tweet['id_str']\n",
    "\n",
    "\n",
    "def verbose_yield(func):\n",
    "    def in_fn(*args, **kwargs):\n",
    "        print(f\"{func.__name__} got {list(args)}\") \n",
    "        res = func(*args, **kwargs)\n",
    "        print(res)\n",
    "        yield res\n",
    "    return in_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SigWordGramTracker(st.TokenTracker):\n",
    "    def __init__(self, grams=(2, 3), sig_cutoff=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.grams = grams\n",
    "        self.sig_cutoff = sig_cutoff\n",
    "\n",
    "        # setup SWM coprocess\n",
    "        self.swm_co_gen = self.swm_cp()\n",
    "        self.swm_co_gen.send(None)\n",
    "        \n",
    "        # Create necessary TF-IDF tracking variables\n",
    "        ##\n",
    "        ##\n",
    "        self.word_doc_freq = {}\n",
    "        # for now, accumulate counts forever\n",
    "        # otherwise implement a hist tracker\n",
    "        # of documents\n",
    "\n",
    "        # Hold off reporting n-grams until we learn about the text\n",
    "        # for now, let us hold the first 1000 tweets\n",
    "        self.pre_history = {}\n",
    "        self.doc_count = {}\n",
    "\n",
    "    def ingress(self):\n",
    "        co_gen = self.co_process()\n",
    "        co_gen.send(None)\n",
    "        return co_gen\n",
    "\n",
    "    def update_word_sig(self, words, lang):\n",
    "        if not lang in self.word_doc_freq:\n",
    "            self.word_doc_freq[lang] = {}\n",
    "        for word in set(words):\n",
    "            self.word_doc_freq[lang][word] = self.word_doc_freq[lang].get(word,0) + 1\n",
    "        # could expire old\n",
    "    \n",
    "    def get_sig_words(self, words, lang):\n",
    "        return [word for word in set(words) if self.is_sig_word(word, lang)]\n",
    "\n",
    "    def is_sig_word(self, word, lang):\n",
    "        # is the word infrequent enough?\n",
    "        if self.sig_cutoff is None:\n",
    "            return True\n",
    "        if not lang in self.word_doc_freq:\n",
    "            return True\n",
    "        if not lang in self.doc_count:\n",
    "            return True\n",
    "        return self.word_doc_freq.get(lang, {}).get(word,0)/self.doc_count[lang] < self.sig_cutoff\n",
    "\n",
    "    def send_ngrams(self, sig_words, dtime):\n",
    "        for n in self.grams:\n",
    "            for ngram in itr.combinations(sorted(sig_words), n):\n",
    "                self.swm_co_gen.send((ngram, dtime))\n",
    "\n",
    "    def co_process(self):\n",
    "        while True:\n",
    "            (lang, words), dtime = (yield)\n",
    "            if not words:\n",
    "                continue\n",
    "\n",
    "            self.doc_count[lang] = self.doc_count.get(lang,0) + 1\n",
    "            if not lang in self.pre_history:\n",
    "                self.pre_history[lang] = []\n",
    "            self.update_word_sig(words, lang)\n",
    "\n",
    "            if self.doc_count[lang] < 1000:\n",
    "                self.pre_history[lang].append((words, dtime))\n",
    "                continue\n",
    "\n",
    "            elif self.doc_count[lang] == 1000:\n",
    "                for words, dtime in self.pre_history[lang]:\n",
    "                    sig_words = self.get_sig_words(words, lang)\n",
    "                    self.send_ngrams(sig_words, dtime)\n",
    "                del self.pre_history[lang]\n",
    "            \n",
    "            else:\n",
    "                sig_words = self.get_sig_words(words, lang)\n",
    "                self.send_ngrams(sig_words, dtime)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#infile_string = \"/Users/cjc73/Downloads/tweets_gnip_latest_2018_7_linejson.bz2\"\n",
    "#infile_string = '/Users/cjc73/Expire/tweet_stream/baton_rouge_twitter.json'\n",
    "infile_string = '/Users/cjc73/Downloads/bad_linejson.bz2'\n",
    "in_file_path = os.path.realpath(infile_string)\n",
    "out_file_path = ''.join(os.path.splitext(in_file_path)[:-1]) + u'.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "##########################\n",
    "# Build a extract-track-report SWM pipeline and process data\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Create report consumers\n",
    "tag_rpt = st.SimpleAccumulator() # printing=True)\n",
    "taggram_rpt = st.SimpleAccumulator()\n",
    "url_rpt = st.SimpleAccumulator()\n",
    "ngram_rpt = st.SimpleAccumulator()\n",
    "mention_rpt = st.SimpleAccumulator()\n",
    "\n",
    "# Extra reporters for retweets\n",
    "rt_tag_rpt = st.SimpleAccumulator()\n",
    "#rt_url_rpt = st.SimpleAccumulator()\n",
    "#rt_ngram_rpt = st.SimpleAccumulator()\n",
    "#rt_mention_rpt = st.SimpleAccumulator()\n",
    "\n",
    "# ----------------------------\n",
    "# Create Trackers\n",
    "\n",
    "windows_ms = (\n",
    "    20 * 60 * 1000,      # 20 minutes in ms\n",
    "    60 * 60 * 1000,      # 1 hr in ms\n",
    "    4 * 60 * 60 * 1000   # 4 hrs in ms\n",
    ")\n",
    "report_triggers = [[2], [2], [2]]\n",
    "\n",
    "common_kwargs = {\n",
    "    'windows': windows_ms,\n",
    "    'report_triggers': report_triggers,\n",
    "    'high_val_thresh_fn': lambda x, t_max: math.log2(x/t_max),\n",
    "    'require_run': False,\n",
    "    'track_all_tags': True,\n",
    "    'no_window_overlap': True,\n",
    "    'report_decreases': False,\n",
    "}\n",
    "\n",
    "twt = {\n",
    "    'rt_tag': [[{'minutes': 20}, [2]], [{'hours': 1}, [2]], [{'hours': 4}, [2]]],\n",
    "    'tag':    [[{'minutes': 20}, [2]], [{'hours': 1}, [2]], [{'hours': 4}, [2]]],\n",
    "    'words':  [[{'minutes': 20}, [2]], [{'hours': 1}, [5]], [{'hours': 4}, [10]]],\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# Set up monitoring coprocesses\n",
    "\n",
    "# tags\n",
    "tag_mon = st.TokenTracker(\n",
    "    consumer=tag_rpt.ingress(),\n",
    "    tag_window_triggers=twt['tag'],\n",
    "    **common_kwargs\n",
    ")\n",
    "rt_tag_mon = st.TokenTracker(\n",
    "    consumer=rt_tag_rpt.ingress(),\n",
    "    tag_window_triggers=twt['rt_tag'],\n",
    "    **common_kwargs\n",
    ")\n",
    "\n",
    "# mentions\n",
    "at_men_mon = st.TokenTracker(\n",
    "    consumer=mention_rpt.ingress(),\n",
    "    tag_window_triggers=twt['tag'],\n",
    "    **common_kwargs\n",
    ")\n",
    "\n",
    "# urls\n",
    "url_mon = st.TokenTracker(\n",
    "    consumer=url_rpt.ingress(),\n",
    "    tag_window_triggers=twt['tag'],\n",
    "    **common_kwargs\n",
    ")\n",
    "\n",
    "# words\n",
    "word_gram_mon = SigWordGramTracker(\n",
    "    consumer=ngram_rpt.ingress(),\n",
    "    tag_window_triggers=twt['words'],\n",
    "    grams=(2,),\n",
    "    sig_cutoff=.025,\n",
    "    **common_kwargs\n",
    ")\n",
    "\n",
    "# tag grams\n",
    "tag_gram_mon = st.TokenTracker(\n",
    "    consumer=taggram_rpt.ingress(),\n",
    "    tag_window_triggers=twt['tag'],\n",
    "    **common_kwargs\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Create and prime extractor\n",
    "te_pipelines = [\n",
    "    # tags\n",
    "    {'getter':   tt.get_tags,\n",
    "     'for_each': tag_mon.ingress(),\n",
    "     'for_rt':   rt_tag_mon.ingress()\n",
    "    },\n",
    "    # words\n",
    "    #{'getter':   tt.get_lang_words,\n",
    "    # 'for_each': word_gram_mon.ingress()\n",
    "    #},\n",
    "    # tag grams\n",
    "    {'getter':   tt.get_tag_grams,\n",
    "     'for_each': tag_gram_mon.ingress(),\n",
    "    },\n",
    "    # urls\n",
    "    {'getter':   tt.get_urls,\n",
    "     'for_each': url_mon.ingress(),\n",
    "    },\n",
    "    # mentions\n",
    "    {'getter':   tt.get_mentions,\n",
    "     'for_each': at_men_mon.ingress(),\n",
    "    },    \n",
    "]\n",
    "te = tt.token_extractor(te_pipelines)\n",
    "te.send(None)\n",
    "\n",
    "# -----------------------------\n",
    "# Preprocessor to compute/fix fields\n",
    "pre = tt.pre_process(consumer=te)\n",
    "pre.send(None)\n",
    "\n",
    "# ----------------------------\n",
    "# Clean stream of duplicates and out-of-order entries\n",
    "#  before processing\n",
    "stream_cleaner = st.StreamCleaner(\n",
    "    delay=datetime.timedelta(seconds=1 * 60),\n",
    "    #consumer=te\n",
    "    consumer=pre\n",
    ")\n",
    "# ---------------------------\n",
    "# Parse or skip lines from file:\n",
    "ps = print_stream()\n",
    "ps.send(None)\n",
    "#jparser = tt.json_parser(consumer=ps)\n",
    "jparser = tt.json_parser(consumer=stream_cleaner.ingress())\n",
    "jparser.send(None)\n",
    "\n",
    "# ----------------------------\n",
    "# Push data through pipeline \n",
    "#line_iter = iter(file_streamer(in_file_path, skip=0))\n",
    "#line_iter = iter(linejsoln_file_streamer(in_file_path, skip=0))\n",
    "line_iter = iter(bz2_linejson_file_streamer(in_flile_path, skip=0))\n",
    "\n",
    "data_pusher(line_iter, jparser)\n",
    "# flush queue\n",
    "stream_cleaner.clearq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Report results\n",
    "\n",
    "labeled_mon_rpt_list = [\n",
    "    ('tag',       tag_mon,       tag_rpt     ),\n",
    "    ('rt_tag',    rt_tag_mon,    rt_tag_rpt  ),\n",
    "    ('tag_gram',  tag_gram_mon,  taggram_rpt ),\n",
    "    ('url',       url_mon,       url_rpt     ),\n",
    "    ('mention',   at_men_mon,    mention_rpt ),\n",
    "    ('word_gram', word_gram_mon, ngram_rpt   ),\n",
    "]\n",
    "\n",
    "for label, mon, rpt in labeled_mon_rpt_list:\n",
    "    print(\"Total {}s seen: {}\".format(label, len(mon.swm.total_tag_count)))\n",
    "    print(\"Total {} uses: {}\".format(label, sum(mon.swm.total_tag_count.values())))\n",
    "\n",
    "    token_counter = coll.Counter(mon.swm.total_tag_count)\n",
    "    print(\"**10 Most Common {}s:**\".format(label))\n",
    "    for tag, count in token_counter.most_common(10):\n",
    "        print(\"\\t{} : {}\".format(tag, count))\n",
    "    \n",
    "    rpt.create_df()\n",
    "    report_counter = coll.Counter(rpt.df['tag'].values)\n",
    "    print(\"**10 Most Reported {}s:**\".format(label))\n",
    "    for tag, count in report_counter.most_common(10):\n",
    "        print(\"\\t{} : {}\".format(tag, count))\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "if out_file_path:\n",
    "    name, ext = os.path.splitext(out_file_path)\n",
    "    tag_outpath = '{}_{}_reports{}'.format(name, label, ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
